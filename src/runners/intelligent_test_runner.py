#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½æµ‹è¯•è¿è¡Œå™¨ - AIé©±åŠ¨çš„æµ‹è¯•æ‰§è¡Œå™¨

é›†æˆAIå†³ç­–èƒ½åŠ›ï¼Œå®ç°æ™ºèƒ½åŒ–çš„æµ‹è¯•è°ƒåº¦ã€æ‰§è¡Œå’Œä¼˜åŒ–
"""

import asyncio
import time
from datetime import datetime
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
import sys
from pathlib import Path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.runners.test_runner import TestRunner
from src.ai.ai_decision_center import AIDecisionCenter, Decision

logger = logging.getLogger(__name__)


@dataclass
class IntelligentTestResult:
    """æ™ºèƒ½æµ‹è¯•ç»“æœ"""
    test_result: Dict[str, Any]
    ai_insights: Dict[str, Any]
    optimization_suggestions: List[Dict[str, Any]]
    execution_metrics: Dict[str, Any]
    decision_tracking: List[str]


class IntelligentTestScheduler:
    """æ™ºèƒ½æµ‹è¯•è°ƒåº¦å™¨"""
    
    def __init__(self, ai_decision_center: AIDecisionCenter):
        self.ai_decision_center = ai_decision_center
        
    async def create_optimal_schedule(self, test_suite: Dict[str, Any], analysis: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºæœ€ä¼˜æµ‹è¯•è°ƒåº¦"""
        context = {
            "operation_type": "test_scheduling",
            "test_count": len(test_suite.get("tests", [])),
            "complexity": analysis.get("complexity", "medium"),
            "estimated_duration": analysis.get("estimated_duration", 300),
            "resource_constraints": analysis.get("constraints", {})
        }
        
        # AIå†³ç­–æœ€ä¼˜è°ƒåº¦ç­–ç•¥
        decision = await self.ai_decision_center.make_intelligent_decision(context)
        
        # åŸºäºAIæ¨èåˆ›å»ºè°ƒåº¦
        schedule = self._create_schedule_from_decision(test_suite, decision)
        
        return {
            "schedule": schedule,
            "ai_decision": decision,
            "optimization_applied": True
        }
    
    def _create_schedule_from_decision(self, test_suite: Dict[str, Any], decision: Decision) -> Dict[str, Any]:
        """åŸºäºAIå†³ç­–åˆ›å»ºè°ƒåº¦"""
        tests = test_suite.get("tests", [])
        
        # è·å–AIæ¨èçš„é…ç½®
        recommended_action = decision.recommendations[0]["action"] if decision.recommendations else {}
        
        parallel_workers = recommended_action.get("parallel_workers", 4)
        timeout = recommended_action.get("timeout", 30)
        
        # æ™ºèƒ½åˆ†ç»„æµ‹è¯•
        test_groups = self._group_tests_intelligently(tests, parallel_workers)
        
        return {
            "test_groups": test_groups,
            "parallel_workers": parallel_workers,
            "timeout_per_test": timeout,
            "retry_failed": True,
            "ai_optimized": True,
            "estimated_total_time": self._estimate_total_time(test_groups, timeout)
        }
    
    def _group_tests_intelligently(self, tests: List[Dict], workers: int) -> List[List[Dict]]:
        """æ™ºèƒ½åˆ†ç»„æµ‹è¯•"""
        # æŒ‰å¤æ‚åº¦æ’åº
        sorted_tests = sorted(tests, key=lambda t: t.get("complexity_score", 1), reverse=True)
        
        # åˆ†ç»„ç­–ç•¥ï¼šå¤æ‚æµ‹è¯•åˆ†æ•£åˆ°ä¸åŒç»„
        groups = [[] for _ in range(workers)]
        group_loads = [0] * workers
        
        for test in sorted_tests:
            # é€‰æ‹©è´Ÿè½½æœ€è½»çš„ç»„
            min_load_group = min(range(workers), key=lambda i: group_loads[i])
            groups[min_load_group].append(test)
            group_loads[min_load_group] += test.get("complexity_score", 1)
        
        return [group for group in groups if group]  # è¿‡æ»¤ç©ºç»„
    
    def _estimate_total_time(self, test_groups: List[List[Dict]], timeout: int) -> int:
        """ä¼°ç®—æ€»æ‰§è¡Œæ—¶é—´"""
        max_group_time = 0
        for group in test_groups:
            group_time = sum(test.get("estimated_time", timeout) for test in group)
            max_group_time = max(max_group_time, group_time)
        return max_group_time


class AdaptiveTestExecutor:
    """è‡ªé€‚åº”æµ‹è¯•æ‰§è¡Œå™¨"""
    
    def __init__(self, ai_decision_center: AIDecisionCenter):
        self.ai_decision_center = ai_decision_center
        self.execution_metrics = {}
        
    async def execute_with_adaptation(self, schedule: Dict[str, Any], analysis: Dict[str, Any]) -> Dict[str, Any]:
        """è‡ªé€‚åº”æ‰§è¡Œæµ‹è¯•"""
        execution_start = time.time()
        results = []
        adaptations_made = []
        
        test_groups = schedule["schedule"]["test_groups"]
        parallel_workers = schedule["schedule"]["parallel_workers"]
        
        logger.info(f"å¼€å§‹æ™ºèƒ½æµ‹è¯•æ‰§è¡Œ: {len(test_groups)} ä¸ªç»„, {parallel_workers} å¹¶å‘")
        
        # å¹¶å‘æ‰§è¡Œæµ‹è¯•ç»„
        tasks = []
        for i, group in enumerate(test_groups):
            task = asyncio.create_task(
                self._execute_test_group(group, i, schedule["schedule"])
            )
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ç»„å®Œæˆï¼ŒåŒæ—¶ç›‘æ§å’Œè‡ªé€‚åº”
        group_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        for i, result in enumerate(group_results):
            if isinstance(result, Exception):
                logger.error(f"æµ‹è¯•ç»„ {i} æ‰§è¡Œå¤±è´¥: {result}")
                # AIå†³ç­–å¦‚ä½•å¤„ç†å¤±è´¥
                adaptation = await self._handle_execution_failure(i, result, schedule)
                adaptations_made.append(adaptation)
            else:
                results.append(result)
        
        execution_time = time.time() - execution_start
        
        return {
            "results": results,
            "execution_time": execution_time,
            "adaptations_made": adaptations_made,
            "ai_optimized": True,
            "schedule_used": schedule
        }
    
    async def _execute_test_group(self, group: List[Dict], group_id: int, schedule_config: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæµ‹è¯•ç»„"""
        logger.info(f"æ‰§è¡Œæµ‹è¯•ç»„ {group_id}: {len(group)} ä¸ªæµ‹è¯•")
        
        group_results = []
        start_time = time.time()
        
        for test in group:
            try:
                # æ‰§è¡Œå•ä¸ªæµ‹è¯•ï¼ˆè¿™é‡Œç®€åŒ–ä¸ºæ¨¡æ‹Ÿï¼‰
                test_result = await self._execute_single_test(test, schedule_config)
                group_results.append(test_result)
                
                # å®æ—¶ç›‘æ§å’Œè‡ªé€‚åº”
                if self._should_adapt(test_result):
                    adaptation = await self._adapt_execution_strategy(test_result, schedule_config)
                    if adaptation:
                        schedule_config.update(adaptation)
                        logger.info(f"åº”ç”¨è‡ªé€‚åº”è°ƒæ•´: {adaptation}")
                
            except Exception as e:
                logger.error(f"æµ‹è¯• {test.get('name', 'unknown')} æ‰§è¡Œå¤±è´¥: {e}")
                group_results.append({
                    "test_name": test.get("name", "unknown"),
                    "success": False,
                    "error": str(e),
                    "execution_time": 0
                })
        
        execution_time = time.time() - start_time
        
        return {
            "group_id": group_id,
            "results": group_results,
            "execution_time": execution_time,
            "success_rate": sum(1 for r in group_results if r.get("success", False)) / len(group_results)
        }
    
    async def _execute_single_test(self, test: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªæµ‹è¯•ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        test_name = test.get("name", "unknown")
        complexity = test.get("complexity_score", 1)
        
        # æ¨¡æ‹Ÿæµ‹è¯•æ‰§è¡Œæ—¶é—´
        execution_time = complexity * config.get("timeout_per_test", 30) / 10
        await asyncio.sleep(min(execution_time, 5))  # æœ€å¤šç­‰å¾…5ç§’ï¼ˆæ¼”ç¤ºç”¨ï¼‰
        
        # æ¨¡æ‹ŸæˆåŠŸç‡ï¼ˆå¤æ‚åº¦è¶Šé«˜ï¼Œå¤±è´¥æ¦‚ç‡è¶Šå¤§ï¼‰
        success_probability = max(0.7, 1 - complexity * 0.1)
        success = time.time() % 1 < success_probability
        
        return {
            "test_name": test_name,
            "success": success,
            "execution_time": execution_time,
            "response_time": execution_time * 0.8,
            "complexity": complexity
        }
    
    def _should_adapt(self, test_result: Dict[str, Any]) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦è‡ªé€‚åº”è°ƒæ•´"""
        # å¦‚æœæµ‹è¯•å¤±è´¥æˆ–æ‰§è¡Œæ—¶é—´è¿‡é•¿ï¼Œè€ƒè™‘è°ƒæ•´
        if not test_result.get("success", True):
            return True
        
        if test_result.get("execution_time", 0) > 60:  # è¶…è¿‡1åˆ†é’Ÿ
            return True
        
        return False
    
    async def _adapt_execution_strategy(self, test_result: Dict[str, Any], current_config: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """è‡ªé€‚åº”è°ƒæ•´æ‰§è¡Œç­–ç•¥"""
        context = {
            "operation_type": "execution_adaptation",
            "test_result": test_result,
            "current_config": current_config,
            "adaptation_trigger": "performance_issue" if test_result.get("execution_time", 0) > 60 else "failure"
        }
        
        # AIå†³ç­–å¦‚ä½•è°ƒæ•´
        decision = await self.ai_decision_center.make_intelligent_decision(context)
        
        if decision.recommendations:
            adaptation = decision.recommendations[0]["action"]
            logger.info(f"AIå»ºè®®è‡ªé€‚åº”è°ƒæ•´: {adaptation}")
            return adaptation
        
        return None
    
    async def _handle_execution_failure(self, group_id: int, error: Exception, schedule: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†æ‰§è¡Œå¤±è´¥"""
        context = {
            "operation_type": "failure_handling",
            "group_id": group_id,
            "error_type": type(error).__name__,
            "error_message": str(error),
            "schedule": schedule
        }
        
        # AIå†³ç­–å¦‚ä½•å¤„ç†å¤±è´¥
        decision = await self.ai_decision_center.make_intelligent_decision(context)
        
        return {
            "group_id": group_id,
            "adaptation_applied": decision.recommendations[0] if decision.recommendations else None,
            "ai_reasoning": decision.reasoning
        }


class IntelligentTestRunner(TestRunner):
    """æ™ºèƒ½æµ‹è¯•è¿è¡Œå™¨ - å…·å¤‡AIå†³ç­–èƒ½åŠ›çš„æµ‹è¯•æ‰§è¡Œå™¨"""
    
    def __init__(self, config_path: Optional[str] = None):
        super().__init__()
        self.ai_decision_center = AIDecisionCenter()
        self.intelligent_scheduler = IntelligentTestScheduler(self.ai_decision_center)
        self.adaptive_executor = AdaptiveTestExecutor(self.ai_decision_center)
        self.ai_active = False
        
    async def start_ai_engine(self):
        """å¯åŠ¨AIå¼•æ“"""
        if not self.ai_active:
            await self.ai_decision_center.start()
            self.ai_active = True
            logger.info("AIæµ‹è¯•å¼•æ“å·²å¯åŠ¨")
    
    def stop_ai_engine(self):
        """åœæ­¢AIå¼•æ“"""
        if self.ai_active:
            self.ai_decision_center.stop()
            self.ai_active = False
            logger.info("AIæµ‹è¯•å¼•æ“å·²åœæ­¢")
    
    async def intelligent_run(self, test_suite: Dict[str, Any]) -> IntelligentTestResult:
        """æ™ºèƒ½æµ‹è¯•æ‰§è¡Œ"""
        if not self.ai_active:
            await self.start_ai_engine()
        
        execution_start = time.time()
        decision_tracking = []
        
        try:
            # 1. AIåˆ†ææµ‹è¯•å¥—ä»¶
            logger.info("AIåˆ†ææµ‹è¯•å¥—ä»¶...")
            analysis = await self._ai_analyze_test_suite(test_suite)
            decision_tracking.append(f"æµ‹è¯•å¥—ä»¶åˆ†æå®Œæˆ: å¤æ‚åº¦ {analysis['complexity']}")
            
            # 2. æ™ºèƒ½è°ƒåº¦æµ‹è¯•
            logger.info("æ™ºèƒ½è°ƒåº¦æµ‹è¯•...")
            schedule_result = await self.intelligent_scheduler.create_optimal_schedule(test_suite, analysis)
            decision_tracking.append(f"æ™ºèƒ½è°ƒåº¦å®Œæˆ: {schedule_result['ai_decision'].confidence:.2f} ç½®ä¿¡åº¦")
            
            # 3. è‡ªé€‚åº”æ‰§è¡Œ
            logger.info("è‡ªé€‚åº”æ‰§è¡Œæµ‹è¯•...")
            execution_result = await self.adaptive_executor.execute_with_adaptation(schedule_result, analysis)
            decision_tracking.append(f"æ‰§è¡Œå®Œæˆ: {len(execution_result['adaptations_made'])} æ¬¡è‡ªé€‚åº”è°ƒæ•´")
            
            # 4. AIç»“æœåˆ†æ
            logger.info("AIåˆ†ææµ‹è¯•ç»“æœ...")
            ai_insights = await self._ai_analyze_results(execution_result, analysis)
            decision_tracking.append("AIæ´å¯Ÿåˆ†æå®Œæˆ")
            
            # 5. ç”Ÿæˆä¼˜åŒ–å»ºè®®
            optimization_suggestions = await self._generate_optimization_suggestions(
                execution_result, analysis, ai_insights
            )
            decision_tracking.append(f"ç”Ÿæˆ {len(optimization_suggestions)} æ¡ä¼˜åŒ–å»ºè®®")
            
            total_execution_time = time.time() - execution_start
            
            return IntelligentTestResult(
                test_result=execution_result,
                ai_insights=ai_insights,
                optimization_suggestions=optimization_suggestions,
                execution_metrics={
                    "total_execution_time": total_execution_time,
                    "ai_decision_time": analysis.get("decision_time", 0),
                    "adaptation_count": len(execution_result.get("adaptations_made", [])),
                    "ai_confidence": schedule_result["ai_decision"].confidence
                },
                decision_tracking=decision_tracking
            )
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            raise
    
    async def _ai_analyze_test_suite(self, test_suite: Dict[str, Any]) -> Dict[str, Any]:
        """AIåˆ†ææµ‹è¯•å¥—ä»¶"""
        tests = test_suite.get("tests", [])
        
        context = {
            "operation_type": "test_suite_analysis",
            "test_count": len(tests),
            "test_types": list(set(test.get("type", "unknown") for test in tests)),
            "has_complex_tests": any(test.get("complexity_score", 1) > 3 for test in tests)
        }
        
        decision_start = time.time()
        decision = await self.ai_decision_center.make_intelligent_decision(context)
        decision_time = time.time() - decision_start
        
        # è®¡ç®—å¤æ‚åº¦
        avg_complexity = sum(test.get("complexity_score", 1) for test in tests) / len(tests) if tests else 1
        
        if avg_complexity > 3:
            complexity = "high"
        elif avg_complexity > 2:
            complexity = "medium"
        else:
            complexity = "low"
        
        return {
            "complexity": complexity,
            "test_count": len(tests),
            "estimated_duration": len(tests) * 30,  # æ¯ä¸ªæµ‹è¯•30ç§’ä¼°ç®—
            "ai_decision": decision,
            "decision_time": decision_time,
            "constraints": {
                "memory_intensive": avg_complexity > 2.5,
                "network_dependent": True
            }
        }
    
    async def _ai_analyze_results(self, execution_result: Dict[str, Any], analysis: Dict[str, Any]) -> Dict[str, Any]:
        """AIåˆ†ææµ‹è¯•ç»“æœ"""
        results = execution_result.get("results", [])
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_tests = sum(len(group["results"]) for group in results)
        successful_tests = sum(
            sum(1 for test in group["results"] if test.get("success", False))
            for group in results
        )
        success_rate = successful_tests / total_tests if total_tests > 0 else 0
        
        # AIæ´å¯Ÿåˆ†æ
        insights = {
            "overall_success_rate": success_rate,
            "total_tests": total_tests,
            "execution_efficiency": self._calculate_efficiency(execution_result, analysis),
            "failure_patterns": self._analyze_failure_patterns(results),
            "performance_insights": self._analyze_performance(results),
            "ai_value_added": {
                "adaptations_successful": len(execution_result.get("adaptations_made", [])) > 0,
                "schedule_optimized": execution_result.get("ai_optimized", False),
                "predicted_vs_actual": self._compare_prediction_vs_actual(analysis, execution_result)
            }
        }
        
        return insights
    
    async def _generate_optimization_suggestions(
        self, 
        execution_result: Dict[str, Any], 
        analysis: Dict[str, Any], 
        insights: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        # åŸºäºæˆåŠŸç‡çš„å»ºè®®
        if insights["overall_success_rate"] < 0.8:
            suggestions.append({
                "type": "quality_improvement",
                "priority": "high",
                "title": "æå‡æµ‹è¯•æˆåŠŸç‡",
                "description": f"å½“å‰æˆåŠŸç‡ {insights['overall_success_rate']:.1%}ï¼Œå»ºè®®ä¼˜åŒ–å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹",
                "action": "review_failed_tests"
            })
        
        # åŸºäºæ€§èƒ½çš„å»ºè®®
        if insights["execution_efficiency"] < 0.7:
            suggestions.append({
                "type": "performance_optimization",
                "priority": "medium",
                "title": "ä¼˜åŒ–æ‰§è¡Œæ•ˆç‡",
                "description": f"æ‰§è¡Œæ•ˆç‡ {insights['execution_efficiency']:.1%}ï¼Œå»ºè®®è°ƒæ•´å¹¶å‘é…ç½®",
                "action": "optimize_parallelism"
            })
        
        # åŸºäºAIè‡ªé€‚åº”çš„å»ºè®®
        if len(execution_result.get("adaptations_made", [])) > 3:
            suggestions.append({
                "type": "stability_improvement",
                "priority": "medium",
                "title": "æå‡æ‰§è¡Œç¨³å®šæ€§",
                "description": "æ‰§è¡Œè¿‡ç¨‹ä¸­è¿›è¡Œäº†å¤šæ¬¡è‡ªé€‚åº”è°ƒæ•´ï¼Œå»ºè®®ä¼˜åŒ–åˆå§‹é…ç½®",
                "action": "improve_initial_config"
            })
        
        return suggestions
    
    def _calculate_efficiency(self, execution_result: Dict[str, Any], analysis: Dict[str, Any]) -> float:
        """è®¡ç®—æ‰§è¡Œæ•ˆç‡"""
        actual_time = execution_result.get("execution_time", 0)
        estimated_time = analysis.get("estimated_duration", 1)
        
        if actual_time <= estimated_time:
            return 1.0
        else:
            return estimated_time / actual_time
    
    def _analyze_failure_patterns(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æå¤±è´¥æ¨¡å¼"""
        failures = []
        for group in results:
            for test in group["results"]:
                if not test.get("success", True):
                    failures.append(test)
        
        if not failures:
            return {"pattern": "no_failures", "count": 0}
        
        # ç®€å•çš„å¤±è´¥æ¨¡å¼åˆ†æ
        error_types = {}
        for failure in failures:
            error = failure.get("error", "unknown")
            error_types[error] = error_types.get(error, 0) + 1
        
        most_common = max(error_types.items(), key=lambda x: x[1]) if error_types else ("none", 0)
        
        return {
            "pattern": most_common[0],
            "count": len(failures),
            "distribution": error_types
        }
    
    def _analyze_performance(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½è¡¨ç°"""
        all_execution_times = []
        for group in results:
            for test in group["results"]:
                if test.get("execution_time"):
                    all_execution_times.append(test["execution_time"])
        
        if not all_execution_times:
            return {"average_time": 0, "max_time": 0, "min_time": 0}
        
        return {
            "average_time": sum(all_execution_times) / len(all_execution_times),
            "max_time": max(all_execution_times),
            "min_time": min(all_execution_times),
            "time_distribution": "normal" if max(all_execution_times) / min(all_execution_times) < 3 else "varied"
        }
    
    def _compare_prediction_vs_actual(self, analysis: Dict[str, Any], execution_result: Dict[str, Any]) -> Dict[str, Any]:
        """æ¯”è¾ƒé¢„æµ‹ä¸å®é™…ç»“æœ"""
        predicted_time = analysis.get("estimated_duration", 0)
        actual_time = execution_result.get("execution_time", 0)
        
        accuracy = 1 - abs(predicted_time - actual_time) / max(predicted_time, 1)
        
        return {
            "time_prediction_accuracy": max(0, accuracy),
            "predicted_time": predicted_time,
            "actual_time": actual_time
        }


# æ¼”ç¤ºå‡½æ•°
async def demo_intelligent_test_runner():
    """æ¼”ç¤ºæ™ºèƒ½æµ‹è¯•è¿è¡Œå™¨"""
    print("ğŸ¤– æ™ºèƒ½æµ‹è¯•è¿è¡Œå™¨æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºæ™ºèƒ½æµ‹è¯•è¿è¡Œå™¨
    runner = IntelligentTestRunner()
    
    try:
        # æ¨¡æ‹Ÿæµ‹è¯•å¥—ä»¶
        test_suite = {
            "name": "ç”µå•†APIæµ‹è¯•å¥—ä»¶",
            "tests": [
                {"name": "ç”¨æˆ·ç™»å½•æµ‹è¯•", "type": "auth", "complexity_score": 2},
                {"name": "å•†å“æŸ¥è¯¢æµ‹è¯•", "type": "query", "complexity_score": 1},
                {"name": "è®¢å•åˆ›å»ºæµ‹è¯•", "type": "transaction", "complexity_score": 3},
                {"name": "æ”¯ä»˜å¤„ç†æµ‹è¯•", "type": "payment", "complexity_score": 4},
                {"name": "åº“å­˜æ›´æ–°æµ‹è¯•", "type": "update", "complexity_score": 2},
            ]
        }
        
        print(f"ğŸ“‹ æµ‹è¯•å¥—ä»¶: {test_suite['name']}")
        print(f"ğŸ”¢ æµ‹è¯•æ•°é‡: {len(test_suite['tests'])}")
        
        # æ‰§è¡Œæ™ºèƒ½æµ‹è¯•
        result = await runner.intelligent_run(test_suite)
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\nâœ… æ™ºèƒ½æµ‹è¯•æ‰§è¡Œå®Œæˆ!")
        print(f"ğŸ“Š æ€»æ‰§è¡Œæ—¶é—´: {result.execution_metrics['total_execution_time']:.2f}ç§’")
        print(f"ğŸ¯ AIç½®ä¿¡åº¦: {result.execution_metrics['ai_confidence']:.2f}")
        print(f"ğŸ”„ è‡ªé€‚åº”è°ƒæ•´: {result.execution_metrics['adaptation_count']}æ¬¡")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {result.ai_insights['overall_success_rate']:.1%}")
        
        print("\nğŸ§  AIå†³ç­–è·Ÿè¸ª:")
        for i, decision in enumerate(result.decision_tracking, 1):
            print(f"   {i}. {decision}")
        
        print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®: {len(result.optimization_suggestions)}æ¡")
        for suggestion in result.optimization_suggestions:
            print(f"   â€¢ {suggestion['title']} ({suggestion['priority']}ä¼˜å…ˆçº§)")
            
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºæ‰§è¡Œå¤±è´¥: {e}")
    finally:
        runner.stop_ai_engine()


if __name__ == "__main__":
    import asyncio
    asyncio.run(demo_intelligent_test_runner())