#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIæµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨ - æ™ºèƒ½åŒ–æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ

ğŸ§  AIä½“ç°ï¼š
- æ™ºèƒ½æ´å¯Ÿåˆ†æï¼šä»æµ‹è¯•ç»“æœä¸­æå–æ·±åº¦æ´å¯Ÿ
- é¢„æµ‹æ€§åˆ†æï¼šé¢„æµ‹æ½œåœ¨é—®é¢˜å’Œä¼˜åŒ–æ–¹å‘  
- è‡ªåŠ¨åŒ–å»ºè®®ï¼šåŸºäºæµ‹è¯•ç»“æœç”Ÿæˆæ™ºèƒ½å»ºè®®
- è¶‹åŠ¿åˆ†æï¼šåˆ†ææµ‹è¯•è´¨é‡è¶‹åŠ¿å’Œæ¨¡å¼
"""

import json
import time
from datetime import datetime
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)


@dataclass
class AIInsight:
    """AIæ´å¯Ÿ"""
    insight_type: str
    title: str
    description: str
    impact: str  # low, medium, high, critical
    confidence: float
    recommendation: str
    data_evidence: Dict[str, Any]


@dataclass
class IntelligentTestReport:
    """æ™ºèƒ½æµ‹è¯•æŠ¥å‘Š"""
    basic_report: Dict[str, Any]
    ai_insights: List[AIInsight]
    trend_analysis: Dict[str, Any]
    predictive_analysis: Dict[str, Any]
    optimization_suggestions: List[Dict[str, Any]]
    quality_score: float
    report_metadata: Dict[str, Any]


class AITestReporter:
    """AIæµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨
    
    ğŸ¤– æ ¸å¿ƒAIèƒ½åŠ›ï¼š
    - æ·±åº¦æ•°æ®åˆ†æå’Œæ¨¡å¼è¯†åˆ«
    - é¢„æµ‹æ€§è´¨é‡è¯„ä¼°
    - æ™ºèƒ½åŒ–ä¼˜åŒ–å»ºè®®ç”Ÿæˆ
    - è‡ªåŠ¨åŒ–æ´å¯Ÿæå–
    """
    
    def __init__(self):
        self.historical_data = []
        self.quality_patterns = {}
        self.performance_baselines = {}
    
    def generate_intelligent_report(self, test_results: List[Dict[str, Any]], 
                                  execution_context: Dict[str, Any]) -> IntelligentTestReport:
        """ç”Ÿæˆæ™ºèƒ½æµ‹è¯•æŠ¥å‘Š
        
        ğŸ§  AIä½“ç°ï¼š
        - è‡ªåŠ¨åˆ†ææµ‹è¯•ç»“æœæ¨¡å¼
        - ç”Ÿæˆæ·±åº¦æ´å¯Ÿå’Œå»ºè®®
        - é¢„æµ‹è´¨é‡è¶‹åŠ¿
        """
        # 1. åŸºç¡€æŠ¥å‘Šæ•°æ®
        basic_report = self._generate_basic_report(test_results, execution_context)
        
        # 2. AIæ·±åº¦æ´å¯Ÿåˆ†æ
        ai_insights = self._generate_ai_insights(test_results, execution_context)
        
        # 3. è¶‹åŠ¿åˆ†æ
        trend_analysis = self._analyze_quality_trends(test_results)
        
        # 4. é¢„æµ‹æ€§åˆ†æ
        predictive_analysis = self._perform_predictive_analysis(test_results, execution_context)
        
        # 5. ä¼˜åŒ–å»ºè®®
        optimization_suggestions = self._generate_optimization_suggestions(
            test_results, ai_insights, trend_analysis
        )
        
        # 6. è´¨é‡è¯„åˆ†
        quality_score = self._calculate_quality_score(test_results, ai_insights)
        
        # 7. æŠ¥å‘Šå…ƒæ•°æ®
        report_metadata = {
            "generated_at": datetime.now().isoformat(),
            "ai_version": "2.0.0",
            "analysis_depth": "comprehensive",
            "confidence_level": self._calculate_overall_confidence(ai_insights)
        }
        
        return IntelligentTestReport(
            basic_report=basic_report,
            ai_insights=ai_insights,
            trend_analysis=trend_analysis,
            predictive_analysis=predictive_analysis,
            optimization_suggestions=optimization_suggestions,
            quality_score=quality_score,
            report_metadata=report_metadata
        )
    
    def _generate_basic_report(self, test_results: List[Dict[str, Any]], 
                             execution_context: Dict[str, Any]) -> Dict[str, Any]:
        """ç”ŸæˆåŸºç¡€æŠ¥å‘Šæ•°æ®"""
        total_tests = len(test_results)
        if total_tests == 0:
            return {"error": "æ— æµ‹è¯•ç»“æœ"}
        
        # ç»Ÿè®¡åŸºç¡€æŒ‡æ ‡
        passed_tests = sum(1 for result in test_results if result.get("success", False))
        failed_tests = total_tests - passed_tests
        success_rate = passed_tests / total_tests
        
        # æ€§èƒ½æŒ‡æ ‡
        response_times = [r.get("response_time", 0) for r in test_results if r.get("response_time")]
        avg_response_time = sum(response_times) / len(response_times) if response_times else 0
        max_response_time = max(response_times) if response_times else 0
        min_response_time = min(response_times) if response_times else 0
        
        return {
            "summary": {
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "failed_tests": failed_tests,
                "success_rate": success_rate,
                "execution_time": execution_context.get("execution_time", 0)
            },
            "performance": {
                "avg_response_time": avg_response_time,
                "max_response_time": max_response_time,
                "min_response_time": min_response_time,
                "performance_score": self._calculate_performance_score(response_times)
            },
            "test_details": test_results
        }
    
    def _generate_ai_insights(self, test_results: List[Dict[str, Any]], 
                            execution_context: Dict[str, Any]) -> List[AIInsight]:
        """ç”ŸæˆAIæ´å¯Ÿ
        
        ğŸ” AIåˆ†æç»´åº¦ï¼š
        - å¤±è´¥æ¨¡å¼è¯†åˆ«
        - æ€§èƒ½å¼‚å¸¸æ£€æµ‹
        - è´¨é‡è¶‹åŠ¿é¢„æµ‹
        - é£é™©ç‚¹è¯†åˆ«
        """
        insights = []
        
        # 1. å¤±è´¥æ¨¡å¼åˆ†æ
        failure_insights = self._analyze_failure_patterns(test_results)
        insights.extend(failure_insights)
        
        # 2. æ€§èƒ½å¼‚å¸¸åˆ†æ
        performance_insights = self._analyze_performance_anomalies(test_results)
        insights.extend(performance_insights)
        
        # 3. è´¨é‡ç¨³å®šæ€§åˆ†æ
        stability_insights = self._analyze_quality_stability(test_results)
        insights.extend(stability_insights)
        
        # 4. æµ‹è¯•è¦†ç›–åº¦åˆ†æ
        coverage_insights = self._analyze_test_coverage(test_results, execution_context)
        insights.extend(coverage_insights)
        
        return insights
    
    def _analyze_failure_patterns(self, test_results: List[Dict[str, Any]]) -> List[AIInsight]:
        """åˆ†æå¤±è´¥æ¨¡å¼"""
        insights = []
        failed_tests = [r for r in test_results if not r.get("success", True)]
        
        if not failed_tests:
            insights.append(AIInsight(
                insight_type="quality",
                title="ğŸ‰ é›¶å¤±è´¥ç‡è¡¨ç°",
                description="æ‰€æœ‰æµ‹è¯•éƒ½æˆåŠŸé€šè¿‡ï¼Œå±•ç°äº†ä¼˜ç§€çš„è´¨é‡ç¨³å®šæ€§",
                impact="low",
                confidence=1.0,
                recommendation="ä¿æŒå½“å‰çš„è´¨é‡æ ‡å‡†ï¼Œè€ƒè™‘å¢åŠ æ›´å¤šè¾¹ç•Œæµ‹è¯•",
                data_evidence={"success_rate": 1.0, "total_tests": len(test_results)}
            ))
            return insights
        
        # åˆ†æå¤±è´¥ç±»å‹
        error_types = {}
        for test in failed_tests:
            error = test.get("error", "æœªçŸ¥é”™è¯¯")
            error_types[error] = error_types.get(error, 0) + 1
        
        # è¯†åˆ«ä¸»è¦å¤±è´¥åŸå› 
        if error_types:
            most_common_error = max(error_types.items(), key=lambda x: x[1])
            failure_rate = len(failed_tests) / len(test_results)
            
            if failure_rate > 0.2:  # å¤±è´¥ç‡è¶…è¿‡20%
                insights.append(AIInsight(
                    insight_type="quality_issue",
                    title="âš ï¸ é«˜å¤±è´¥ç‡é¢„è­¦",
                    description=f"æµ‹è¯•å¤±è´¥ç‡è¾¾åˆ°{failure_rate:.1%}ï¼Œä¸»è¦åŸå› ï¼š{most_common_error[0]}",
                    impact="high",
                    confidence=0.9,
                    recommendation="ä¼˜å…ˆä¿®å¤ä¸»è¦å¤±è´¥åŸå› ï¼Œæ£€æŸ¥ç³»ç»Ÿç¨³å®šæ€§",
                    data_evidence={
                        "failure_rate": failure_rate,
                        "main_error": most_common_error[0],
                        "error_count": most_common_error[1],
                        "error_distribution": error_types
                    }
                ))
            elif most_common_error[1] >= 3:  # åŒç±»é”™è¯¯â‰¥3æ¬¡
                insights.append(AIInsight(
                    insight_type="pattern_detection",
                    title="ğŸ” é‡å¤é”™è¯¯æ¨¡å¼",
                    description=f"æ£€æµ‹åˆ°é‡å¤çš„é”™è¯¯æ¨¡å¼ï¼š{most_common_error[0]}",
                    impact="medium",
                    confidence=0.8,
                    recommendation="åˆ†æè¯¥é”™è¯¯çš„æ ¹æœ¬åŸå› ï¼Œå®æ–½é’ˆå¯¹æ€§ä¿®å¤",
                    data_evidence={
                        "error_type": most_common_error[0],
                        "occurrence_count": most_common_error[1],
                        "affected_tests": [t.get("test_name", "") for t in failed_tests 
                                         if t.get("error") == most_common_error[0]]
                    }
                ))
        
        return insights
    
    def _analyze_performance_anomalies(self, test_results: List[Dict[str, Any]]) -> List[AIInsight]:
        """åˆ†ææ€§èƒ½å¼‚å¸¸"""
        insights = []
        response_times = [r.get("response_time", 0) for r in test_results if r.get("response_time")]
        
        if not response_times:
            return insights
        
        avg_response_time = sum(response_times) / len(response_times)
        max_response_time = max(response_times)
        
        # æ£€æµ‹æ€§èƒ½å¼‚å¸¸
        if max_response_time > avg_response_time * 3:  # æœ€å¤§å“åº”æ—¶é—´è¶…è¿‡å¹³å‡å€¼3å€
            # æ‰¾å‡ºå¼‚å¸¸çš„æµ‹è¯•
            anomaly_threshold = avg_response_time * 2
            slow_tests = [
                r for r in test_results 
                if r.get("response_time", 0) > anomaly_threshold
            ]
            
            insights.append(AIInsight(
                insight_type="performance_anomaly",
                title="âš¡ æ€§èƒ½å¼‚å¸¸æ£€æµ‹",
                description=f"æ£€æµ‹åˆ°{len(slow_tests)}ä¸ªå¼‚å¸¸æ…¢çš„æµ‹è¯•ï¼Œæœ€å¤§å“åº”æ—¶é—´{max_response_time:.2f}s",
                impact="medium",
                confidence=0.85,
                recommendation="åˆ†ææ…¢æµ‹è¯•çš„å…±åŒç‰¹å¾ï¼Œä¼˜åŒ–æ€§èƒ½ç“¶é¢ˆ",
                data_evidence={
                    "avg_response_time": avg_response_time,
                    "max_response_time": max_response_time,
                    "slow_tests_count": len(slow_tests),
                    "slow_tests": [t.get("test_name", "") for t in slow_tests]
                }
            ))
        
        # æ£€æµ‹æ•´ä½“æ€§èƒ½æ°´å¹³
        if avg_response_time > 5.0:  # å¹³å‡å“åº”æ—¶é—´è¶…è¿‡5ç§’
            insights.append(AIInsight(
                insight_type="performance_degradation",
                title="ğŸŒ æ•´ä½“æ€§èƒ½ä¸‹é™",
                description=f"å¹³å‡å“åº”æ—¶é—´{avg_response_time:.2f}sï¼Œè¶…å‡ºç†æƒ³èŒƒå›´",
                impact="high",
                confidence=0.9,
                recommendation="æ£€æŸ¥ç³»ç»Ÿè´Ÿè½½ã€ç½‘ç»œçŠ¶å†µå’Œæ¥å£ä¼˜åŒ–æœºä¼š",
                data_evidence={
                    "avg_response_time": avg_response_time,
                    "baseline": 5.0,
                    "degradation_ratio": avg_response_time / 5.0
                }
            ))
        elif avg_response_time < 1.0:  # å¹³å‡å“åº”æ—¶é—´å°äº1ç§’
            insights.append(AIInsight(
                insight_type="performance_excellence",
                title="ğŸš€ ä¼˜ç§€æ€§èƒ½è¡¨ç°",
                description=f"å¹³å‡å“åº”æ—¶é—´{avg_response_time:.2f}sï¼Œè¡¨ç°ä¼˜ç§€",
                impact="low",
                confidence=0.9,
                recommendation="ä¿æŒå½“å‰æ€§èƒ½æ°´å¹³ï¼Œå¯è€ƒè™‘å¢åŠ æ›´å¤šæµ‹è¯•è¦†ç›–",
                data_evidence={
                    "avg_response_time": avg_response_time,
                    "performance_level": "excellent"
                }
            ))
        
        return insights
    
    def _analyze_quality_stability(self, test_results: List[Dict[str, Any]]) -> List[AIInsight]:
        """åˆ†æè´¨é‡ç¨³å®šæ€§"""
        insights = []
        
        # è®¡ç®—è´¨é‡æŒ‡æ ‡
        success_rate = sum(1 for r in test_results if r.get("success", False)) / len(test_results)
        
        # åˆ†æè´¨é‡ç­‰çº§
        if success_rate >= 0.98:
            insights.append(AIInsight(
                insight_type="quality_excellent",
                title="ğŸ’ å“è¶Šè´¨é‡æ°´å¹³",
                description=f"æˆåŠŸç‡{success_rate:.1%}ï¼Œè¾¾åˆ°å“è¶Šè´¨é‡æ ‡å‡†",
                impact="low",
                confidence=0.95,
                recommendation="ä¿æŒé«˜è´¨é‡æ ‡å‡†ï¼Œç»§ç»­ä¼˜åŒ–æµ‹è¯•ç­–ç•¥",
                data_evidence={"success_rate": success_rate, "quality_level": "excellent"}
            ))
        elif success_rate >= 0.90:
            insights.append(AIInsight(
                insight_type="quality_good",
                title="âœ… è‰¯å¥½è´¨é‡æ°´å¹³",
                description=f"æˆåŠŸç‡{success_rate:.1%}ï¼Œè´¨é‡æ°´å¹³è‰¯å¥½",
                impact="low",
                confidence=0.9,
                recommendation="ç»§ç»­ä¿æŒï¼Œå¯é’ˆå¯¹å¤±è´¥æµ‹è¯•è¿›è¡Œä¼˜åŒ–",
                data_evidence={"success_rate": success_rate, "quality_level": "good"}
            ))
        elif success_rate >= 0.80:
            insights.append(AIInsight(
                insight_type="quality_warning",
                title="âš ï¸ è´¨é‡éœ€è¦å…³æ³¨",
                description=f"æˆåŠŸç‡{success_rate:.1%}ï¼Œè´¨é‡æ°´å¹³éœ€è¦æ”¹è¿›",
                impact="medium",
                confidence=0.9,
                recommendation="é‡ç‚¹åˆ†æå¤±è´¥åŸå› ï¼Œåˆ¶å®šè´¨é‡æ”¹è¿›è®¡åˆ’",
                data_evidence={"success_rate": success_rate, "quality_level": "needs_improvement"}
            ))
        else:
            insights.append(AIInsight(
                insight_type="quality_critical",
                title="ğŸš¨ è´¨é‡ä¸¥é‡é—®é¢˜",
                description=f"æˆåŠŸç‡{success_rate:.1%}ï¼Œè´¨é‡æ°´å¹³ä¸¥é‡ä¸è¶³",
                impact="critical",
                confidence=0.95,
                recommendation="ç«‹å³åœæ­¢å‘å¸ƒï¼Œç´§æ€¥ä¿®å¤è´¨é‡é—®é¢˜",
                data_evidence={"success_rate": success_rate, "quality_level": "critical"}
            ))
        
        return insights
    
    def _analyze_test_coverage(self, test_results: List[Dict[str, Any]], 
                             execution_context: Dict[str, Any]) -> List[AIInsight]:
        """åˆ†ææµ‹è¯•è¦†ç›–åº¦"""
        insights = []
        
        # åˆ†æAPIè¦†ç›–æƒ…å†µ
        api_count = execution_context.get("api_count", 0)
        tested_apis = len(test_results)
        
        if api_count > 0:
            coverage_rate = tested_apis / api_count
            
            if coverage_rate < 0.7:  # è¦†ç›–ç‡ä½äº70%
                insights.append(AIInsight(
                    insight_type="coverage_gap",
                    title="ğŸ“Š æµ‹è¯•è¦†ç›–ä¸è¶³",
                    description=f"APIè¦†ç›–ç‡{coverage_rate:.1%}ï¼Œå­˜åœ¨è¦†ç›–ç¼ºå£",
                    impact="medium",
                    confidence=0.8,
                    recommendation="å¢åŠ æµ‹è¯•ç”¨ä¾‹ä»¥æé«˜APIè¦†ç›–ç‡",
                    data_evidence={
                        "coverage_rate": coverage_rate,
                        "total_apis": api_count,
                        "tested_apis": tested_apis,
                        "uncovered_apis": api_count - tested_apis
                    }
                ))
            elif coverage_rate >= 0.9:  # è¦†ç›–ç‡>=90%
                insights.append(AIInsight(
                    insight_type="coverage_excellent",
                    title="ğŸ¯ ä¼˜ç§€æµ‹è¯•è¦†ç›–",
                    description=f"APIè¦†ç›–ç‡{coverage_rate:.1%}ï¼Œæµ‹è¯•è¦†ç›–åº¦ä¼˜ç§€",
                    impact="low",
                    confidence=0.9,
                    recommendation="ä¿æŒé«˜è¦†ç›–ç‡ï¼Œå¯è€ƒè™‘å¢åŠ è¾¹ç•Œå’Œå¼‚å¸¸æµ‹è¯•",
                    data_evidence={
                        "coverage_rate": coverage_rate,
                        "total_apis": api_count,
                        "tested_apis": tested_apis
                    }
                ))
        
        return insights
    
    def _analyze_quality_trends(self, test_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æè´¨é‡è¶‹åŠ¿"""
        # ç®€åŒ–çš„è¶‹åŠ¿åˆ†æ
        current_success_rate = sum(1 for r in test_results if r.get("success", False)) / len(test_results)
        
        # ä¸å†å²æ•°æ®æ¯”è¾ƒï¼ˆç®€åŒ–å®ç°ï¼‰
        historical_avg = 0.85  # å‡è®¾çš„å†å²å¹³å‡å€¼
        
        trend = "stable"
        if current_success_rate > historical_avg + 0.05:
            trend = "improving"
        elif current_success_rate < historical_avg - 0.05:
            trend = "declining"
        
        return {
            "current_success_rate": current_success_rate,
            "historical_average": historical_avg,
            "trend": trend,
            "trend_confidence": 0.7,
            "prediction": {
                "next_period_estimate": current_success_rate,
                "confidence_interval": [current_success_rate - 0.05, current_success_rate + 0.05]
            }
        }
    
    def _perform_predictive_analysis(self, test_results: List[Dict[str, Any]], 
                                   execution_context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œé¢„æµ‹æ€§åˆ†æ"""
        # åŸºäºå½“å‰ç»“æœé¢„æµ‹æœªæ¥è¶‹åŠ¿
        current_metrics = {
            "success_rate": sum(1 for r in test_results if r.get("success", False)) / len(test_results),
            "avg_response_time": sum(r.get("response_time", 0) for r in test_results) / len(test_results),
            "failure_count": sum(1 for r in test_results if not r.get("success", True))
        }
        
        # ç®€åŒ–çš„é¢„æµ‹æ¨¡å‹
        predictions = {
            "quality_forecast": {
                "next_week": current_metrics["success_rate"],
                "confidence": 0.7,
                "factors": ["å½“å‰è´¨é‡ç¨³å®šæ€§", "å†å²è¶‹åŠ¿"]
            },
            "performance_forecast": {
                "trend": "stable",
                "estimated_degradation": 0.02,  # 2%æ€§èƒ½ä¸‹é™é¢„æµ‹
                "confidence": 0.6
            },
            "risk_assessment": {
                "failure_probability": max(0.1, current_metrics["failure_count"] / len(test_results)),
                "risk_level": "low" if current_metrics["success_rate"] > 0.9 else "medium",
                "mitigation_priority": ["æ€§èƒ½ä¼˜åŒ–", "é”™è¯¯å¤„ç†", "ç›‘æ§å¢å¼º"]
            }
        }
        
        return predictions
    
    def _generate_optimization_suggestions(self, test_results: List[Dict[str, Any]], 
                                         insights: List[AIInsight],
                                         trend_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        # åŸºäºæ´å¯Ÿç”Ÿæˆå»ºè®®
        high_impact_insights = [i for i in insights if i.impact in ["high", "critical"]]
        
        for insight in high_impact_insights:
            suggestions.append({
                "type": "urgent_action",
                "title": f"ç´§æ€¥ä¼˜åŒ–ï¼š{insight.title}",
                "description": insight.recommendation,
                "priority": "high",
                "estimated_impact": insight.impact,
                "confidence": insight.confidence
            })
        
        # åŸºäºè¶‹åŠ¿ç”Ÿæˆå»ºè®®
        if trend_analysis["trend"] == "declining":
            suggestions.append({
                "type": "trend_improvement",
                "title": "è´¨é‡è¶‹åŠ¿æ”¹è¿›",
                "description": "è´¨é‡æŒ‡æ ‡å‘ˆä¸‹é™è¶‹åŠ¿ï¼Œå»ºè®®åŠ å¼ºè´¨é‡ç›‘æ§å’Œæµ‹è¯•ç­–ç•¥",
                "priority": "medium",
                "estimated_impact": "high",
                "confidence": 0.8
            })
        
        # æ€§èƒ½ä¼˜åŒ–å»ºè®®
        response_times = [r.get("response_time", 0) for r in test_results if r.get("response_time")]
        if response_times:
            avg_time = sum(response_times) / len(response_times)
            if avg_time > 3.0:
                suggestions.append({
                    "type": "performance_optimization",
                    "title": "æ€§èƒ½ä¼˜åŒ–æœºä¼š",
                    "description": f"å¹³å‡å“åº”æ—¶é—´{avg_time:.2f}sï¼Œå»ºè®®ä¼˜åŒ–æ¥å£æ€§èƒ½",
                    "priority": "medium",
                    "estimated_impact": "medium",
                    "confidence": 0.7
                })
        
        return suggestions
    
    def _calculate_quality_score(self, test_results: List[Dict[str, Any]], 
                               insights: List[AIInsight]) -> float:
        """è®¡ç®—è´¨é‡è¯„åˆ†"""
        base_score = 100.0
        
        # åŸºäºæˆåŠŸç‡æ‰£åˆ†
        success_rate = sum(1 for r in test_results if r.get("success", False)) / len(test_results)
        base_score *= success_rate
        
        # åŸºäºæ´å¯Ÿæ‰£åˆ†
        for insight in insights:
            if insight.impact == "critical":
                base_score -= 20
            elif insight.impact == "high":
                base_score -= 10
            elif insight.impact == "medium":
                base_score -= 5
        
        # åŸºäºæ€§èƒ½åŠ åˆ†/æ‰£åˆ†
        response_times = [r.get("response_time", 0) for r in test_results if r.get("response_time")]
        if response_times:
            avg_time = sum(response_times) / len(response_times)
            if avg_time < 1.0:
                base_score += 5  # æ€§èƒ½ä¼˜ç§€åŠ åˆ†
            elif avg_time > 5.0:
                base_score -= 10  # æ€§èƒ½å·®æ‰£åˆ†
        
        return max(0, min(100, base_score))
    
    def _calculate_performance_score(self, response_times: List[float]) -> float:
        """è®¡ç®—æ€§èƒ½è¯„åˆ†"""
        if not response_times:
            return 50.0
        
        avg_time = sum(response_times) / len(response_times)
        
        # æ€§èƒ½è¯„åˆ†ç®—æ³•
        if avg_time <= 1.0:
            return 100.0
        elif avg_time <= 2.0:
            return 85.0
        elif avg_time <= 5.0:
            return 70.0
        elif avg_time <= 10.0:
            return 50.0
        else:
            return 30.0
    
    def _calculate_overall_confidence(self, insights: List[AIInsight]) -> float:
        """è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦"""
        if not insights:
            return 0.5
        
        confidences = [insight.confidence for insight in insights]
        return sum(confidences) / len(confidences)
    
    def export_intelligent_report(self, report: IntelligentTestReport, 
                                format_type: str = "json") -> str:
        """å¯¼å‡ºæ™ºèƒ½æŠ¥å‘Š"""
        if format_type == "json":
            return self._export_json_report(report)
        elif format_type == "html":
            return self._export_html_report(report)
        elif format_type == "markdown":
            return self._export_markdown_report(report)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æŠ¥å‘Šæ ¼å¼: {format_type}")
    
    def _export_json_report(self, report: IntelligentTestReport) -> str:
        """å¯¼å‡ºJSONæ ¼å¼æŠ¥å‘Š"""
        report_dict = {
            "basic_report": report.basic_report,
            "ai_insights": [
                {
                    "type": insight.insight_type,
                    "title": insight.title,
                    "description": insight.description,
                    "impact": insight.impact,
                    "confidence": insight.confidence,
                    "recommendation": insight.recommendation,
                    "data_evidence": insight.data_evidence
                }
                for insight in report.ai_insights
            ],
            "trend_analysis": report.trend_analysis,
            "predictive_analysis": report.predictive_analysis,
            "optimization_suggestions": report.optimization_suggestions,
            "quality_score": report.quality_score,
            "metadata": report.report_metadata
        }
        
        return json.dumps(report_dict, ensure_ascii=False, indent=2)
    
    def _export_markdown_report(self, report: IntelligentTestReport) -> str:
        """å¯¼å‡ºMarkdownæ ¼å¼æŠ¥å‘Š"""
        md_content = f"""# ğŸ¤– AIæ™ºèƒ½æµ‹è¯•æŠ¥å‘Š

## ğŸ“Š åŸºç¡€ç»Ÿè®¡

- **æ€»æµ‹è¯•æ•°**: {report.basic_report['summary']['total_tests']}
- **æˆåŠŸæµ‹è¯•**: {report.basic_report['summary']['passed_tests']}
- **å¤±è´¥æµ‹è¯•**: {report.basic_report['summary']['failed_tests']}
- **æˆåŠŸç‡**: {report.basic_report['summary']['success_rate']:.1%}
- **è´¨é‡è¯„åˆ†**: {report.quality_score:.1f}/100

## ğŸ§  AIæ´å¯Ÿåˆ†æ

"""
        
        for insight in report.ai_insights:
            impact_emoji = {"low": "ğŸŸ¢", "medium": "ğŸŸ¡", "high": "ğŸ”´", "critical": "ğŸš¨"}
            md_content += f"""### {insight.title}

- **å½±å“çº§åˆ«**: {impact_emoji.get(insight.impact, "âšª")} {insight.impact}
- **ç½®ä¿¡åº¦**: {insight.confidence:.1%}
- **æè¿°**: {insight.description}
- **å»ºè®®**: {insight.recommendation}

"""
        
        md_content += f"""## ğŸ“ˆ è¶‹åŠ¿åˆ†æ

- **å½“å‰æˆåŠŸç‡**: {report.trend_analysis['current_success_rate']:.1%}
- **å†å²å¹³å‡**: {report.trend_analysis['historical_average']:.1%}
- **è¶‹åŠ¿**: {report.trend_analysis['trend']}

## ğŸ”® é¢„æµ‹åˆ†æ

- **è´¨é‡é¢„æµ‹**: {report.predictive_analysis['quality_forecast']['next_week']:.1%}
- **é£é™©ç­‰çº§**: {report.predictive_analysis['risk_assessment']['risk_level']}

## ğŸ’¡ ä¼˜åŒ–å»ºè®®

"""
        
        for i, suggestion in enumerate(report.optimization_suggestions, 1):
            md_content += f"{i}. **{suggestion['title']}** ({suggestion['priority']}ä¼˜å…ˆçº§)\n   - {suggestion['description']}\n\n"
        
        md_content += f"""---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {report.report_metadata['generated_at']}*  
*AIç‰ˆæœ¬: {report.report_metadata['ai_version']}*
"""
        
        return md_content
    
    def _export_html_report(self, report: IntelligentTestReport) -> str:
        """å¯¼å‡ºHTMLæ ¼å¼æŠ¥å‘Šï¼ˆç®€åŒ–å®ç°ï¼‰"""
        return f"""<!DOCTYPE html>
<html>
<head>
    <title>AIæ™ºèƒ½æµ‹è¯•æŠ¥å‘Š</title>
    <meta charset="utf-8">
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background: #f0f0f0; padding: 20px; border-radius: 5px; }}
        .insight {{ margin: 10px 0; padding: 15px; border-left: 4px solid #007acc; background: #f9f9f9; }}
        .quality-score {{ font-size: 24px; font-weight: bold; color: #007acc; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ¤– AIæ™ºèƒ½æµ‹è¯•æŠ¥å‘Š</h1>
        <div class="quality-score">è´¨é‡è¯„åˆ†: {report.quality_score:.1f}/100</div>
    </div>
    
    <h2>ğŸ“Š åŸºç¡€ç»Ÿè®¡</h2>
    <ul>
        <li>æ€»æµ‹è¯•æ•°: {report.basic_report['summary']['total_tests']}</li>
        <li>æˆåŠŸç‡: {report.basic_report['summary']['success_rate']:.1%}</li>
    </ul>
    
    <h2>ğŸ§  AIæ´å¯Ÿ</h2>
"""
        
        for insight in report.ai_insights:
            html_content += f"""    <div class="insight">
        <h3>{insight.title}</h3>
        <p><strong>æè¿°:</strong> {insight.description}</p>
        <p><strong>å»ºè®®:</strong> {insight.recommendation}</p>
        <p><strong>ç½®ä¿¡åº¦:</strong> {insight.confidence:.1%}</p>
    </div>
"""
        
        html_content += """</body>
</html>"""
        
        return html_content


# æ¼”